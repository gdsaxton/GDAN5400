{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c907206",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gdsaxton/GDAN5400/blob/main/Week%204%20Notebooks/GDAN%205400%20-%20Week%204%20Notebooks%20%28V%29%20-%20Dropping%20Duplicate%20Observations.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58dcaba",
   "metadata": {},
   "source": [
    "This notebook provides recipes for removing duplicate observations from a PANDAS dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9da7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import datetime\n",
    "print (\"Current date and time : \", datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af03858",
   "metadata": {},
   "source": [
    "# Load Packages and Set Working Directory\n",
    "Import several necessary Python packages. We will be using the <a href=\"http://pandas.pydata.org/\">Python Data Analysis Library,</a> or <i>PANDAS</i>, extensively for our data manipulations in this and future tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c2d746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas import Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab477ac",
   "metadata": {},
   "source": [
    "<br>\n",
    "PANDAS allows you to set various options for, among other things, inspecting the data. I like to be able to see all of the columns. Therefore, I typically include this line at the top of all my notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdca7e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://pandas.pydata.org/pandas-docs/stable/options.html\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('max_colwidth', 250)\n",
    "pd.set_option('display.max_info_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b452531",
   "metadata": {},
   "source": [
    "# Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a6165c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# NOTE: replace `https://github.com/` with `https://raw.githubusercontent.com`\n",
    "# https://github.com/gdsaxton/GDAN5400/blob/main/Coding%20Assignment%201/final_insurance_fraud.xlsx\n",
    "url = 'https://raw.githubusercontent.com/gdsaxton/GDAN5400/main/Coding%20Assignment%201/final_insurance_fraud.xlsx'\n",
    "\n",
    "# Download the file\n",
    "response = requests.get(url)\n",
    "with open('final_insurance_fraud.xlsx', 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel('final_insurance_fraud.xlsx', engine='openpyxl')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db5ad45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#APPLY DATA CLEANING OPERATIONS FROM CODING ASSIGNMENT 1\n",
    "df = df[df['Policy Number'].notnull()]\n",
    "df['Estimated cost to repair'] = df['Estimated cost to repair'].fillna(0)\n",
    "df['Estimated cost to replace'] = df['Estimated cost to replace'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ce74df",
   "metadata": {},
   "source": [
    "# Identifying Duplicate Observations\n",
    "In Coding Assignment 1, we **_identified_** duplicate observations using  `duplicated()` \n",
    "  - It is a good practice to first identify duplicates using `duplicated()` and then decide how to handle them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1da4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df[df.duplicated(subset=['House/Apartment Number', 'Street Address', 'City', 'Zip Code'], keep=False)]\n",
    "print(f\"Number of duplicate claims: {len(duplicates)}\") \n",
    "duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6b57db",
   "metadata": {},
   "source": [
    "# Dropping Duplicate Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c58e8bd",
   "metadata": {},
   "source": [
    "**[ChatGPT prompt]** `How can I delete duplicate observations in PANDAS?`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeb0a18",
   "metadata": {},
   "source": [
    "# Removing Duplicate Observations in Pandas\n",
    "\n",
    "The `drop_duplicates()` method in pandas is the primary way to remove duplicate observations. Hereâ€™s a detailed guide:\n",
    "\n",
    "---\n",
    "\n",
    "### Remove All Duplicate Rows\n",
    "```python\n",
    "df = df.drop_duplicates()\n",
    "```\n",
    "\n",
    "This removes any rows that are identical across all columns, keeping the first occurrence.\n",
    "\n",
    "\n",
    "### Specify Columns to Check for Duplicates\n",
    "```python\n",
    "df = df.drop_duplicates(subset=['Column1', 'Column2'])\n",
    "```\n",
    "This removes rows where values in Column1 and Column2 are the same.\n",
    "\n",
    "\n",
    "### Keep the First Occurrence (Default)\n",
    "```python\n",
    "df = df.drop_duplicates(keep='first')\n",
    "```\n",
    "\n",
    "### Keep the Last Occurrence\n",
    "```python\n",
    "df = df.drop_duplicates(keep='last')\n",
    "```\n",
    "\n",
    "### Remove All Occurrences of Duplicates\n",
    "```python\n",
    "df = df.drop_duplicates(keep=False)\n",
    "```\n",
    "\n",
    "### Create a New DataFrame Without Duplicates\n",
    "```python\n",
    "new_df = df.drop_duplicates()\n",
    "```\n",
    "\n",
    "### Modify the Original DataFrame\n",
    "```python\n",
    "df.drop_duplicates(inplace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b9c794",
   "metadata": {},
   "source": [
    "<br>Now let's run this on our dataframe. Note that we are only dropping duplicates based off two columns: `House/Apartment Number` and `Street Address`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2fc6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "df = df.drop_duplicates(subset=['House/Apartment Number', 'Street Address'], keep='first')\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3818583e",
   "metadata": {},
   "source": [
    "<br>We can now re-run the duplicates check and see whether any remain in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0211370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df[df.duplicated(subset=['House/Apartment Number', 'Street Address'], keep=False)]\n",
    "print(f\"Number of duplicate claims: {len(duplicates)}\") \n",
    "duplicates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
